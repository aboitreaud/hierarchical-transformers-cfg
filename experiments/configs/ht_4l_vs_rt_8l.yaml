# HT-4L vs RT-8L Comparison Experiment
experiment_name: "ht_4l_vs_rt_8l"
description: "Compare 4-layer HT vs 8-layer RT with matched parameter count"

# CFG Configuration (using CFG-7L)
cfg:
  L: 7
  ns: [1, 3, 3, 3, 5, 5, 9, 10]
  nr: [2, 2, 2, 2, 2, 2, 2]
  T: [2, 2, 2, 2, 2, 4, 4] # sentence length = 512

# Model Configurations
models:
  ht_4l:
    type: "HierarchicalTransformer"
    n_transformers: 4
    n_layer: 4
    n_head: 6
    head_size: 64
    dropout: 0.0
    bias: false
    
  rt_8l:
    type: "GPT"
    n_layer: 8
    n_head: 6
    head_size: 64
    dropout: 0.0
    bias: false

# Training Configuration
training:
  num_epochs: 100
  batch_size: 100
  batches_per_epoch: 50
  
  # Cosine LR Schedule
  max_lr: 6e-4
  min_lr: 6e-5
  
  # Optimizer
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  
  # Evaluation
  eval_iters: 100
  quality_metric_iters: 100
  eval_interval: 10

# Logging
logging:
  project: "HT-4L vs RT-8L"
  log_interval: 10
  save_checkpoints: true

# Hardware
device: "cuda"
use_wandb: true
